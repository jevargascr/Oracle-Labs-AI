{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f92344cf-07cd-4f2e-a47c-a941fa6da877",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ‚öôÔ∏è Ejercicio 03 ‚Äì Uso de Oracle Generative AI (LLM) con SDK de Python\n",
    "\n",
    "## üéØ Objetivo del laboratorio\n",
    "\n",
    "Este laboratorio muestra c√≥mo **ejecutar solicitudes a modelos de lenguaje (LLM)** disponibles en **Oracle Cloud Infrastructure (OCI)** usando el **SDK de Python** y autenticaci√≥n mediante **Resource Principal** o **archivo de configuraci√≥n local** (`~/.oci/config`).\n",
    "\n",
    "A trav√©s de un ejemplo pr√°ctico, aprender√°s a:\n",
    "\n",
    "* Configurar correctamente el entorno local y las credenciales del SDK.\n",
    "* Crear y verificar la conexi√≥n con el servicio **Generative AI Inference**.\n",
    "* Construir una solicitud (*chat request*) tipo conversaci√≥n con texto.\n",
    "* Invocar un modelo de lenguaje y obtener una respuesta en formato JSON.\n",
    "\n",
    "El objetivo final es entender c√≥mo el SDK gestiona los par√°metros del modelo (tokens, temperatura, `top_p`, etc.) y c√≥mo integrar esta llamada dentro de flujos de inferencia, notebooks o pipelines en OCI Data Science.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Contexto\n",
    "\n",
    "Oracle ofrece una serie de modelos **Generative AI** accesibles por API, compatibles con chat, summarization, embeddings, clasificaci√≥n, entre otros.\n",
    "\n",
    "En este laboratorio se usa un **modelo de tipo ‚ÄúCHAT‚Äù**, identificado mediante su **OCID** (`MODEL_ID`), desplegado en la regi√≥n `us-chicago-1`. El flujo emplea el cliente `GenerativeAiInferenceClient` del SDK `oci`, que permite conectarse directamente al endpoint:\n",
    "\n",
    "```python\n",
    "SERVICE_ENDPOINT = \"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Pasos principales\n",
    "\n",
    "1. **Configuraci√≥n de autenticaci√≥n**\n",
    "\n",
    "   * Crear la carpeta `~/.oci/` y colocar el archivo `config` con los par√°metros: `user`, `tenancy`, `region`, `fingerprint`, `key_file`. # Si ya hiciste el ejercicio 01 omite el primer punto)\n",
    " \n",
    "2. **Inicializaci√≥n del cliente**\n",
    "\n",
    "   * Importar y configurar `GenerativeAiInferenceClient` con el `signer` y el `SERVICE_ENDPOINT`.\n",
    "\n",
    "3. **Creaci√≥n de la solicitud**\n",
    "\n",
    "   * Construir un objeto `GenericChatRequest` con el texto del usuario, los par√°metros del modelo (`temperature`, `max_tokens`, `top_p`, etc.), y el rol `\"USER\"`.\n",
    "\n",
    "4. **Ejecuci√≥n e interpretaci√≥n de la respuesta**\n",
    "\n",
    "   * Enviar la solicitud mediante `inf.chat(chat_detail)` y visualizar la respuesta formateada en JSON.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Ejemplo de salida\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"response\": \"\\\"Llama\\\" puede tener diferentes significados dependiendo del contexto...\"\n",
    "}\n",
    "```\n",
    "\n",
    "El modelo interpreta el prompt textual y devuelve una respuesta estructurada con contexto, demostrando el flujo completo de **pregunta ‚Üí inferencia ‚Üí respuesta generativa**.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf511e6b-8b91-4b99-aeec-fd460c5fd68f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!oci --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc9c515-7202-4941-beb5-890fc0671ddb",
   "metadata": {},
   "source": [
    "## 01 - Configuraci√≥n de la autenticaci√≥n del SDK de OCI ( si ya ejecutaste el ejercicio 01, no es necesario ejecutar estos pasos, puedes pasar al paso 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83589cee-e292-4ab2-9646-1a487ee577bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# crea carpeta y permisos\n",
    "#!mkdir -p /home/datascience/.oci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615111c8-27f5-434b-adb9-6a8a269ca85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver tu HOME y listar (incluye ocultos)\n",
    "#!echo $HOME\n",
    "#!ls -la $HOME | head -n 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5437f8-6732-46d2-8c3a-6d0eac858251",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!mkdir -p ~/.oci\n",
    "#!ls -la ~/.oci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae296113-bb25-46ae-b6ec-2013e2bee3d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# cat > ~/.oci/config <<'CFG'\n",
    "# [DEFAULT]\n",
    "# user=ocid1.user.oc1..\n",
    "# tenancy=ocid1.tenancy.oc1..a\n",
    "# region=us-chicago-1\n",
    "# fingerprint=6d:a2:b6:25:24:40:11:\n",
    "# key_file=/home/datascience/.oci/oci_api_key.pem\n",
    "# CFG\n",
    "\n",
    "# echo \"Config creado en ~/.oci/config\"\n",
    "# cat ~/.oci/config | sed 's/fingerprint=.*/fingerprint=<oculto>/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c1907f-c429-496a-b28d-fa00802f7dbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Quita posibles finales de l√≠nea de Windows (CRLF)\n",
    "# !sed -i 's/\\r$//' ~/.oci/config\n",
    "\n",
    "# # mostrar\n",
    "# !sed -n '1,200p' ~/.oci/config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b745e4a-0c5d-4db5-8ab8-8b020e3d2b38",
   "metadata": {},
   "source": [
    "# 2. Usando los Modelos de LLM usando Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a503bec-6da6-4523-9426-0d88c2c4851c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = \"qu√© es llama?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503f4555-3325-4a1b-b702-94786dec5b2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import oci\n",
    "import json\n",
    "from oci.auth.signers import get_resource_principals_signer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efd943a-4166-460e-ba86-366515ebad39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === Config ===\n",
    "REGION = \"us-chicago-1\"\n",
    "SERVICE_ENDPOINT = f\"https://inference.generativeai.{REGION}.oci.oraclecloud.com\" #NO MODIFICAR\n",
    "COMPARTMENT_ID = \"ocid1.compartment.oc1....\" #PARAMETRO A ACTUALIZAR\n",
    "MODEL_ID = \"ocid1.generativeaimodel.oc1.us-chicago-1.amaaaaaask7dceya6dvgvvj3ovy4lerdl6fvx525x3yweacnrgn4ryfwwcoq\" #NO MODIFICAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90590031-1848-487a-b751-74859289a529",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# === Signer===\n",
    "signer = get_resource_principals_signer()\n",
    "cfg = {\"region\": REGION}\n",
    "\n",
    "# (opcional)\n",
    "if MODEL_ID is None:\n",
    "    from oci.generative_ai import GenerativeAiClient\n",
    "    genai = GenerativeAiClient(config=cfg, signer=signer)\n",
    "    models = genai.list_models(\n",
    "        compartment_id=COMPARTMENT_ID,\n",
    "        capability=[\"CHAT\"],\n",
    "        lifecycle_state=\"ACTIVE\"\n",
    "    ).data.items\n",
    "    assert models, \"No hay modelos CHAT visibles en el compartimento. Revisa permisos/compartimento.\"\n",
    "    MODEL_ID = models[0].id\n",
    "    print(\"Usando modelo:\", MODEL_ID)\n",
    "\n",
    "# === Cliente de inferencia ===\n",
    "inf = oci.generative_ai_inference.GenerativeAiInferenceClient(\n",
    "    config=cfg, signer=signer, service_endpoint=SERVICE_ENDPOINT\n",
    ")\n",
    "\n",
    "# === Prompt del usuario ===\n",
    "user_input = user_input\n",
    "\n",
    "# --- Construcci√≥n del request ---\n",
    "content = oci.generative_ai_inference.models.TextContent(text=user_input)\n",
    "message = oci.generative_ai_inference.models.Message(role=\"USER\", content=[content])\n",
    "\n",
    "chat_request = oci.generative_ai_inference.models.GenericChatRequest(\n",
    "    api_format=oci.generative_ai_inference.models.BaseChatRequest.API_FORMAT_GENERIC,\n",
    "    messages=[message],\n",
    "    max_tokens=600,\n",
    "    temperature=1.0,\n",
    "    frequency_penalty=0.0,\n",
    "    presence_penalty=0.0,\n",
    "    top_p=0.75,\n",
    ")\n",
    "\n",
    "chat_detail = oci.generative_ai_inference.models.ChatDetails(\n",
    "    serving_mode=oci.generative_ai_inference.models.OnDemandServingMode(model_id=MODEL_ID),\n",
    "    chat_request=chat_request,\n",
    "    compartment_id=COMPARTMENT_ID,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d0c4a3-96f3-4298-9e21-04d5f2947783",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === Llamada ===\n",
    "resp = inf.chat(chat_detail)\n",
    "\n",
    "# === Resultado ===\n",
    "choices = resp.data.chat_response.choices\n",
    "response_text = choices[0].message.content[0].text if choices else \"No se gener√≥ respuesta.\"\n",
    "print(json.dumps({\"response\": response_text}, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0516ac33-1d80-43a7-8cd2-1b2a76831b24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
